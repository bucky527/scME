{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install scME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first install scME\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scvi\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "sc.set_figure_params(figsize=(8, 8))\n",
    "from scipy.io import mmread\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import scme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the count data matrix of BMNC dataset\n",
    "#read RNA data\n",
    "rna_count=mmread('./example_data/rnacountdgc.mtx').toarray()\n",
    "genes=pd.read_csv(\"./example_data/genes.csv\",index_col=0)\n",
    "cellids=pd.read_csv(\"./example_data/cellids.csv\",index_col=0)\n",
    "\n",
    "#read protein data\n",
    "protein_count=pd.read_csv('./example_data/adtcount.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_count=pd.DataFrame(rna_count.T,index=cellids.values[:,0],columns=genes.values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/scME/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: X.dtype being converted to np.float32 from int64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/miniconda3/envs/scME/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: X.dtype being converted to np.float32 from int64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#preprocess the data\n",
    "#create adata object\n",
    "rna=ad.AnnData(X=rna_count.values,obs=pd.DataFrame(index=rna_count.index), var=pd.DataFrame(index=rna_count.columns))\n",
    "protein=ad.AnnData(X=protein_count.values,obs=pd.DataFrame(index=protein_count.index), var=pd.DataFrame(index=protein_count.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/scME/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:64: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# select highly variable genes\n",
    "rna.layers[\"counts\"] = rna.X.copy()\n",
    "sc.pp.normalize_total(rna)\n",
    "sc.pp.log1p(rna)\n",
    "sc.pp.highly_variable_genes( \n",
    "    rna,\n",
    "    n_top_genes=2000,\n",
    "    flavor=\"seurat_v3\",\n",
    "    subset = True\n",
    ")\n",
    "rna.raw = rna\n",
    "rna = rna[:, rna.var.highly_variable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA and clustering for RNA data\n",
    "sc.pp.pca(rna,svd_solver='arpack')\n",
    "sc.pp.neighbors(rna, n_neighbors=30,n_pcs=30)   \n",
    "sc.tl.leiden(rna, key_added=\"rna_leiden\",resolution=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clr_normalize_each_cell(adata, inplace=True):\n",
    "    \"\"\"Normalize count vector for each cell, i.e. for each row of .X\"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy\n",
    "\n",
    "    def seurat_clr(x):\n",
    "        # TODO: support sparseness\n",
    "        s = np.sum(np.log1p(x[x > 0]))\n",
    "        exp = np.exp(s / len(x))\n",
    "        return np.log1p(x / exp)\n",
    "\n",
    "    if not inplace:\n",
    "        adata = adata.copy()\n",
    "\n",
    "    # apply to dense or sparse matrix, along axis. returns dense matrix\n",
    "    adata.X = np.apply_along_axis(\n",
    "        seurat_clr, 1, (adata.X.A if scipy.sparse.issparse(adata.X) else adata.X)\n",
    "    )\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein.layers[\"counts\"] = protein.X.copy()\n",
    "protein=clr_normalize_each_cell(protein)\n",
    "sc.pp.pca(protein, svd_solver=\"arpack\")\n",
    "sc.pp.neighbors(protein, n_neighbors=10) \n",
    "sc.tl.leiden(protein, key_added=\"protein_leiden\",resolution=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30672, 2000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training dataset\n",
    "rna.X=rna.layers[\"counts\"]\n",
    "protein.X=protein.layers[\"counts\"]\n",
    "traindataset=scme.AnnDataset(rna,protein,to_onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#built scme model and initial\n",
    "#use negative binomial distribution for protein data\n",
    "model=scme.build_scme(rna,protein,traindataset,protein_dist=\"NB\",if_preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0000  loss:6514751.65236 loss_ae:5776424.6595 loss_cls:738326.9928\n",
      "epoch :0001  loss:4612695.50782 loss_ae:4119352.3385 loss_cls:493343.1693\n",
      "epoch :0002  loss:3719854.26473 loss_ae:3341219.8858 loss_cls:378634.3789\n",
      "epoch :0003  loss:3096637.08078 loss_ae:2792387.0307 loss_cls:304250.0501\n",
      "epoch :0004  loss:2527195.01396 loss_ae:2275028.6511 loss_cls:252166.3628\n",
      "epoch :0005  loss:2098633.11286 loss_ae:1883802.6205 loss_cls:214830.4923\n",
      "epoch :0006  loss:1766582.44572 loss_ae:1578377.0578 loss_cls:188205.3879\n",
      "epoch :0007  loss:1491784.39948 loss_ae:1321008.1255 loss_cls:170776.2740\n",
      "epoch :0008  loss:1247109.55308 loss_ae:1096773.0935 loss_cls:150336.4596\n",
      "epoch :0009  loss:1052366.39135 loss_ae:913130.4210 loss_cls:139235.9703\n",
      "epoch :0010  loss:884849.73785 loss_ae:756996.3891 loss_cls:127853.3487\n",
      "epoch :0011  loss:755204.66203 loss_ae:635933.9592 loss_cls:119270.7029\n",
      "epoch :0012  loss:642545.33583 loss_ae:532676.6289 loss_cls:109868.7069\n",
      "epoch :0013  loss:546937.91944 loss_ae:449249.7105 loss_cls:97688.2089\n",
      "epoch :0014  loss:475504.34449 loss_ae:382926.7512 loss_cls:92577.5933\n",
      "epoch :0015  loss:411249.62984 loss_ae:327079.3364 loss_cls:84170.2934\n",
      "epoch :0016  loss:363151.00503 loss_ae:283563.0206 loss_cls:79587.9844\n",
      "epoch :0017  loss:328502.76562 loss_ae:252679.4659 loss_cls:75823.2998\n",
      "epoch :0018  loss:299990.24749 loss_ae:228524.2961 loss_cls:71465.9514\n",
      "epoch :0019  loss:280466.34312 loss_ae:213865.4778 loss_cls:66600.8653\n",
      "epoch :0020  loss:266416.65506 loss_ae:202507.5753 loss_cls:63909.0797\n",
      "epoch :0021  loss:254180.95821 loss_ae:193209.8043 loss_cls:60971.1539\n",
      "epoch :0022  loss:242853.50758 loss_ae:185382.6916 loss_cls:57470.8159\n",
      "epoch :0023  loss:232130.96461 loss_ae:178310.4505 loss_cls:53820.5141\n",
      "epoch :0024  loss:229140.02715 loss_ae:174932.4870 loss_cls:54207.5401\n",
      "epoch :0025  loss:222858.60291 loss_ae:171170.1780 loss_cls:51688.4249\n",
      "epoch :0026  loss:216693.24146 loss_ae:167023.6587 loss_cls:49669.5828\n",
      "epoch :0027  loss:208965.96248 loss_ae:161983.4448 loss_cls:46982.5177\n",
      "epoch :0028  loss:206976.25009 loss_ae:161209.1828 loss_cls:45767.0673\n",
      "epoch :0029  loss:202656.86843 loss_ae:157756.6327 loss_cls:44900.2357\n",
      "epoch :0030  loss:203899.36534 loss_ae:157345.8269 loss_cls:46553.5385\n",
      "epoch :0031  loss:194985.84761 loss_ae:153511.5529 loss_cls:41474.2947\n",
      "epoch :0032  loss:190811.62151 loss_ae:150711.2363 loss_cls:40100.3852\n",
      "epoch :0033  loss:189695.92182 loss_ae:149839.3661 loss_cls:39856.5558\n",
      "epoch :0034  loss:203378.41055 loss_ae:165300.3518 loss_cls:38078.0587\n",
      "epoch :0035  loss:202956.61089 loss_ae:152727.8416 loss_cls:50228.7693\n",
      "epoch :0036  loss:186546.91900 loss_ae:145895.9224 loss_cls:40650.9966\n",
      "epoch :0037  loss:184980.81097 loss_ae:145807.8070 loss_cls:39173.0039\n",
      "epoch :0038  loss:179571.85661 loss_ae:144132.4019 loss_cls:35439.4547\n",
      "epoch :0039  loss:180641.78262 loss_ae:145030.4372 loss_cls:35611.3454\n",
      "epoch :0040  loss:178121.42602 loss_ae:142760.6926 loss_cls:35360.7335\n",
      "epoch :0041  loss:173402.81032 loss_ae:140376.9810 loss_cls:33025.8294\n",
      "epoch :0042  loss:168380.07549 loss_ae:139088.2504 loss_cls:29291.8251\n",
      "epoch :0043  loss:172161.06643 loss_ae:141202.3857 loss_cls:30958.6807\n",
      "epoch :0044  loss:167841.47342 loss_ae:138402.7169 loss_cls:29438.7565\n",
      "epoch :0045  loss:168872.73864 loss_ae:139723.8704 loss_cls:29148.8682\n",
      "epoch :0046  loss:167232.12377 loss_ae:137066.5877 loss_cls:30165.5360\n",
      "epoch :0047  loss:166306.37395 loss_ae:136049.3250 loss_cls:30257.0490\n",
      "epoch :0048  loss:162711.20545 loss_ae:134515.3161 loss_cls:28195.8893\n",
      "epoch :0049  loss:163456.23319 loss_ae:135377.4282 loss_cls:28078.8050\n",
      "epoch :0050  loss:161171.88947 loss_ae:133987.8320 loss_cls:27184.0575\n",
      "epoch :0051  loss:160956.82631 loss_ae:132722.8572 loss_cls:28233.9691\n",
      "epoch :0052  loss:155375.26143 loss_ae:131368.5847 loss_cls:24006.6768\n",
      "epoch :0053  loss:162316.15991 loss_ae:136190.6302 loss_cls:26125.5297\n",
      "epoch :0054  loss:163667.24822 loss_ae:136297.2422 loss_cls:27370.0061\n",
      "epoch :0055  loss:158935.99396 loss_ae:133132.8785 loss_cls:25803.1154\n",
      "epoch :0056  loss:155127.62108 loss_ae:131639.0220 loss_cls:23488.5991\n",
      "epoch :0057  loss:153057.33725 loss_ae:130108.2767 loss_cls:22949.0606\n",
      "epoch :0058  loss:150479.63066 loss_ae:128424.2667 loss_cls:22055.3640\n",
      "epoch :0059  loss:149676.30798 loss_ae:128029.7561 loss_cls:21646.5519\n",
      "epoch :0060  loss:151679.45030 loss_ae:128529.7808 loss_cls:23149.6695\n",
      "epoch :0061  loss:150450.08855 loss_ae:128038.6170 loss_cls:22411.4716\n",
      "epoch :0062  loss:149817.26215 loss_ae:127566.5184 loss_cls:22250.7438\n",
      "epoch :0063  loss:150681.18282 loss_ae:127625.9495 loss_cls:23055.2333\n",
      "epoch :0064  loss:150152.99942 loss_ae:127978.3011 loss_cls:22174.6983\n",
      "epoch :0065  loss:148376.48525 loss_ae:126815.6122 loss_cls:21560.8730\n",
      "epoch :0066  loss:147520.40847 loss_ae:126387.5734 loss_cls:21132.8350\n",
      "epoch :0067  loss:146011.01162 loss_ae:125384.9847 loss_cls:20626.0269\n",
      "epoch :0068  loss:145521.86422 loss_ae:125046.6545 loss_cls:20475.2097\n",
      "epoch :0069  loss:150881.84953 loss_ae:128706.4334 loss_cls:22175.4161\n",
      "epoch :0070  loss:146240.46271 loss_ae:126005.2738 loss_cls:20235.1889\n",
      "epoch :0071  loss:155305.74583 loss_ae:130823.3523 loss_cls:24482.3935\n",
      "epoch :0072  loss:151980.55389 loss_ae:129683.4012 loss_cls:22297.1527\n",
      "epoch :0073  loss:146564.14410 loss_ae:126331.9174 loss_cls:20232.2267\n",
      "epoch :0074  loss:146743.95801 loss_ae:126523.6767 loss_cls:20220.2813\n",
      "epoch :0075  loss:154814.82391 loss_ae:130135.9020 loss_cls:24678.9219\n",
      "epoch :0076  loss:150283.33573 loss_ae:126660.5482 loss_cls:23622.7875\n",
      "epoch :0077  loss:142009.01485 loss_ae:124749.3309 loss_cls:17259.6839\n",
      "epoch :0078  loss:142620.39750 loss_ae:124751.8429 loss_cls:17868.5546\n",
      "epoch :0079  loss:140056.98493 loss_ae:123999.5154 loss_cls:16057.4695\n",
      "epoch :0080  loss:132741.20755 loss_ae:122266.7153 loss_cls:10474.4922\n",
      "epoch :0081  loss:130162.79441 loss_ae:121305.4679 loss_cls:8857.3265\n",
      "epoch :0082  loss:129222.50669 loss_ae:120944.8418 loss_cls:8277.6649\n",
      "epoch :0083  loss:128343.72142 loss_ae:120796.6225 loss_cls:7547.0989\n",
      "epoch :0084  loss:128224.79897 loss_ae:120400.1640 loss_cls:7824.6350\n",
      "epoch :0085  loss:127525.36887 loss_ae:120288.0073 loss_cls:7237.3616\n",
      "epoch :0086  loss:127809.90773 loss_ae:119990.3188 loss_cls:7819.5889\n",
      "epoch :0087  loss:127002.71045 loss_ae:119887.6092 loss_cls:7115.1013\n",
      "epoch :0088  loss:125911.31956 loss_ae:119592.8353 loss_cls:6318.4843\n",
      "epoch :0089  loss:127016.22191 loss_ae:119839.8566 loss_cls:7176.3653\n",
      "epoch :0090  loss:126713.99057 loss_ae:119570.1730 loss_cls:7143.8175\n",
      "epoch :0091  loss:126321.58732 loss_ae:119462.3049 loss_cls:6859.2824\n",
      "epoch :0092  loss:125994.71974 loss_ae:119271.1251 loss_cls:6723.5946\n",
      "epoch :0093  loss:126316.23135 loss_ae:119205.5549 loss_cls:7110.6764\n",
      "epoch :0094  loss:125728.48436 loss_ae:119000.6445 loss_cls:6727.8399\n",
      "epoch :0095  loss:126548.51972 loss_ae:119349.6258 loss_cls:7198.8939\n",
      "epoch :0096  loss:126266.46627 loss_ae:119174.5569 loss_cls:7091.9094\n",
      "epoch :0097  loss:125650.96668 loss_ae:118881.1062 loss_cls:6769.8605\n",
      "epoch :0098  loss:125467.10426 loss_ae:118818.4685 loss_cls:6648.6358\n",
      "epoch :0099  loss:126043.29548 loss_ae:118706.7455 loss_cls:7336.5500\n",
      "epoch :0100  loss:125083.46038 loss_ae:118555.6406 loss_cls:6527.8198\n",
      "epoch :0101  loss:124875.81844 loss_ae:118546.8654 loss_cls:6328.9531\n",
      "epoch :0102  loss:124911.67893 loss_ae:118326.5553 loss_cls:6585.1236\n",
      "epoch :0103  loss:125379.73984 loss_ae:118249.2990 loss_cls:7130.4408\n",
      "epoch :0104  loss:124335.23437 loss_ae:118138.3081 loss_cls:6196.9263\n",
      "epoch :0105  loss:124839.13268 loss_ae:118030.2865 loss_cls:6808.8462\n",
      "epoch :0106  loss:124775.77108 loss_ae:117953.2900 loss_cls:6822.4811\n",
      "epoch :0107  loss:123606.40696 loss_ae:117893.7850 loss_cls:5712.6220\n",
      "epoch :0108  loss:123744.86133 loss_ae:117697.2927 loss_cls:6047.5687\n",
      "epoch :0109  loss:124132.70312 loss_ae:117775.5088 loss_cls:6357.1944\n",
      "epoch :0110  loss:124172.17751 loss_ae:117696.4159 loss_cls:6475.7616\n",
      "epoch :0111  loss:124512.17378 loss_ae:117624.7091 loss_cls:6887.4647\n",
      "epoch :0112  loss:124626.30927 loss_ae:117465.9803 loss_cls:7160.3289\n",
      "epoch :0113  loss:124233.26493 loss_ae:117324.7007 loss_cls:6908.5643\n",
      "epoch :0114  loss:123970.20604 loss_ae:117216.7408 loss_cls:6753.4652\n",
      "epoch :0115  loss:123740.53241 loss_ae:117261.5635 loss_cls:6478.9689\n",
      "epoch :0116  loss:123172.32172 loss_ae:117119.0431 loss_cls:6053.2786\n",
      "epoch :0117  loss:123128.62324 loss_ae:117015.0437 loss_cls:6113.5795\n",
      "epoch :0118  loss:123768.44724 loss_ae:117306.6907 loss_cls:6461.7565\n",
      "epoch :0119  loss:123870.48535 loss_ae:116946.2848 loss_cls:6924.2005\n",
      "epoch :0120  loss:124327.30971 loss_ae:116903.5498 loss_cls:7423.7599\n",
      "epoch :0121  loss:122842.84340 loss_ae:116629.3845 loss_cls:6213.4589\n",
      "epoch :0122  loss:122413.89349 loss_ae:116563.4628 loss_cls:5850.4307\n",
      "epoch :0123  loss:122769.70761 loss_ae:116495.9690 loss_cls:6273.7386\n",
      "epoch :0124  loss:125435.08821 loss_ae:117992.9581 loss_cls:7442.1301\n",
      "epoch :0125  loss:124315.09556 loss_ae:117475.0113 loss_cls:6840.0843\n",
      "epoch :0126  loss:123046.37749 loss_ae:116857.7838 loss_cls:6188.5937\n",
      "epoch :0127  loss:123590.41100 loss_ae:116706.4162 loss_cls:6883.9948\n",
      "epoch :0128  loss:124700.42601 loss_ae:116953.8296 loss_cls:7746.5964\n",
      "epoch :0129  loss:123384.31339 loss_ae:116833.0778 loss_cls:6551.2356\n",
      "epoch :0130  loss:122982.56640 loss_ae:116973.7853 loss_cls:6008.7811\n",
      "epoch :0131  loss:123289.65238 loss_ae:116578.1616 loss_cls:6711.4908\n",
      "epoch :0132  loss:123204.34833 loss_ae:116468.9899 loss_cls:6735.3584\n",
      "epoch :0133  loss:122110.37984 loss_ae:116282.6468 loss_cls:5827.7331\n",
      "epoch :0134  loss:122800.79726 loss_ae:116302.9326 loss_cls:6497.8647\n",
      "epoch :0135  loss:122951.57017 loss_ae:116110.1052 loss_cls:6841.4650\n",
      "epoch :0136  loss:121298.50941 loss_ae:115797.5392 loss_cls:5500.9702\n",
      "epoch :0137  loss:121919.71646 loss_ae:115817.4848 loss_cls:6102.2316\n",
      "epoch :0138  loss:122317.77246 loss_ae:115803.1863 loss_cls:6514.5862\n",
      "epoch :0139  loss:122255.86462 loss_ae:115721.7742 loss_cls:6534.0905\n",
      "epoch :0140  loss:121729.44970 loss_ae:115693.1159 loss_cls:6036.3338\n",
      "epoch :0141  loss:122250.80519 loss_ae:115892.6279 loss_cls:6358.1773\n",
      "epoch :0142  loss:122300.28932 loss_ae:115837.6262 loss_cls:6462.6631\n",
      "epoch :0143  loss:121597.88722 loss_ae:115719.7532 loss_cls:5878.1340\n",
      "epoch :0144  loss:121592.86340 loss_ae:115566.2991 loss_cls:6026.5643\n",
      "epoch :0145  loss:121239.82590 loss_ae:115426.6013 loss_cls:5813.2246\n",
      "epoch :0146  loss:122417.49800 loss_ae:115524.2057 loss_cls:6893.2923\n",
      "epoch :0147  loss:122659.38848 loss_ae:116051.9879 loss_cls:6607.4006\n",
      "epoch :0148  loss:121779.87788 loss_ae:115774.1925 loss_cls:6005.6854\n",
      "epoch :0149  loss:121938.00614 loss_ae:115498.1027 loss_cls:6439.9035\n",
      "epoch :0150  loss:122369.52915 loss_ae:115605.9364 loss_cls:6763.5927\n",
      "epoch :0151  loss:121433.21390 loss_ae:115326.3116 loss_cls:6106.9023\n",
      "epoch :0152  loss:121934.60818 loss_ae:115274.3757 loss_cls:6660.2325\n",
      "epoch :0153  loss:122344.50789 loss_ae:115214.8943 loss_cls:7129.6136\n",
      "epoch :0154  loss:121024.33617 loss_ae:115078.3986 loss_cls:5945.9375\n",
      "epoch :0155  loss:121461.47340 loss_ae:115065.4000 loss_cls:6396.0734\n",
      "epoch :0156  loss:122491.86361 loss_ae:115609.7727 loss_cls:6882.0909\n",
      "epoch :0157  loss:121481.94566 loss_ae:115240.1885 loss_cls:6241.7571\n",
      "epoch :0158  loss:120686.97375 loss_ae:114981.1595 loss_cls:5705.8142\n",
      "epoch :0159  loss:120774.73221 loss_ae:114861.3428 loss_cls:5913.3894\n",
      "epoch :0160  loss:121140.03005 loss_ae:114900.0038 loss_cls:6240.0262\n",
      "epoch :0161  loss:121397.44890 loss_ae:114887.4888 loss_cls:6509.9602\n",
      "epoch :0162  loss:120939.45904 loss_ae:114822.9670 loss_cls:6116.4920\n",
      "epoch :0163  loss:121128.87496 loss_ae:114771.1602 loss_cls:6357.7147\n",
      "epoch :0164  loss:122401.92454 loss_ae:114907.6497 loss_cls:7494.2748\n",
      "epoch :0165  loss:120538.38762 loss_ae:114680.3051 loss_cls:5858.0825\n",
      "epoch :0166  loss:121274.89413 loss_ae:114658.2849 loss_cls:6616.6092\n",
      "epoch :0167  loss:121252.10679 loss_ae:114536.5103 loss_cls:6715.5965\n",
      "epoch :0168  loss:121187.83512 loss_ae:114408.3440 loss_cls:6779.4911\n",
      "epoch :0169  loss:120610.48858 loss_ae:114441.7433 loss_cls:6168.7453\n",
      "epoch :0170  loss:120803.55443 loss_ae:114353.2294 loss_cls:6450.3250\n",
      "epoch :0171  loss:121121.91983 loss_ae:114232.5097 loss_cls:6889.4101\n",
      "epoch :0172  loss:122790.54194 loss_ae:115176.9684 loss_cls:7613.5735\n",
      "epoch :0173  loss:121462.59726 loss_ae:114741.8508 loss_cls:6720.7464\n",
      "epoch :0174  loss:120630.57209 loss_ae:114396.4021 loss_cls:6234.1699\n",
      "epoch :0175  loss:120599.34122 loss_ae:114167.9178 loss_cls:6431.4234\n",
      "epoch :0176  loss:120063.49380 loss_ae:114141.8007 loss_cls:5921.6931\n",
      "epoch :0177  loss:120350.31770 loss_ae:114148.3144 loss_cls:6202.0033\n",
      "epoch :0178  loss:121492.79103 loss_ae:114077.5765 loss_cls:7415.2146\n",
      "epoch :0179  loss:120185.38428 loss_ae:114126.6342 loss_cls:6058.7501\n",
      "epoch :0180  loss:120155.01992 loss_ae:113937.9145 loss_cls:6217.1054\n",
      "epoch :0181  loss:121178.56880 loss_ae:114463.3359 loss_cls:6715.2329\n",
      "epoch :0182  loss:120156.95116 loss_ae:114038.2218 loss_cls:6118.7293\n",
      "epoch :0183  loss:119976.42555 loss_ae:114065.0830 loss_cls:5911.3426\n",
      "epoch :0184  loss:120259.49620 loss_ae:113941.0783 loss_cls:6318.4179\n",
      "epoch :0185  loss:120597.47771 loss_ae:113857.8786 loss_cls:6739.5991\n",
      "epoch :0186  loss:120639.67203 loss_ae:114046.0667 loss_cls:6593.6053\n",
      "epoch :0187  loss:120269.18417 loss_ae:113923.6520 loss_cls:6345.5321\n",
      "epoch :0188  loss:120030.85547 loss_ae:113989.2918 loss_cls:6041.5637\n",
      "epoch :0189  loss:119966.90694 loss_ae:114267.6841 loss_cls:5699.2229\n",
      "epoch :0190  loss:120818.90546 loss_ae:113932.8733 loss_cls:6886.0321\n",
      "epoch :0191  loss:120735.23594 loss_ae:114313.0140 loss_cls:6422.2219\n",
      "epoch :0192  loss:120457.98947 loss_ae:113956.8715 loss_cls:6501.1180\n",
      "epoch :0193  loss:119985.66613 loss_ae:113786.8240 loss_cls:6198.8421\n",
      "epoch :0194  loss:120212.33470 loss_ae:113650.6742 loss_cls:6561.6605\n",
      "epoch :0195  loss:118898.42211 loss_ae:113541.3820 loss_cls:5357.0401\n",
      "epoch :0196  loss:119760.77767 loss_ae:113422.1048 loss_cls:6338.6728\n",
      "epoch :0197  loss:119515.46136 loss_ae:113491.3175 loss_cls:6024.1438\n",
      "epoch :0198  loss:119555.07863 loss_ae:113586.9728 loss_cls:5968.1058\n",
      "epoch :0199  loss:119609.27555 loss_ae:113479.4015 loss_cls:6129.8741\n"
     ]
    }
   ],
   "source": [
    "#train the scme model\n",
    "model=scme.train_model(model,max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the embedding using scme model inference encoder\n",
    "import torch\n",
    "rnatorch,proteintorch=torch.from_numpy(np.array(rna.X)),torch.from_numpy(np.array(protein.X))\n",
    "rnatorch,proteintorch=rnatorch.to(model.device),proteintorch.to(model.device)\n",
    "model.eval()\n",
    "zm=model.inference(rnatorch, proteintorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('scME')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f621d9d21ddc07ff29714a29f4a409099da1514b85b2a58d1fa4c654581448f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
